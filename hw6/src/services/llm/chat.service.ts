import { LLMClient, LLMMessage } from "@/lib/llm/client";
import { OpenAIClient } from "@/lib/llm/openai";
import { handleLLMError, getFallbackResponse } from "@/lib/llm/fallback";
import { PromptService } from "./prompt.service";
import { Logger } from "@/lib/utils/logger";

export class ChatService {
  private llmClient: LLMClient;

  constructor() {
    try {
      this.llmClient = new OpenAIClient();
    } catch (error) {
      Logger.error("Failed to initialize LLM client", { error });
      throw error;
    }
  }

  async generateResponse(
    userMessage: string,
    history: Array<{ role: string; content: string }> = []
  ): Promise<string> {
    try {
      console.log("ğŸ“ [ChatService] é–‹å§‹ç”Ÿæˆå›æ‡‰");
      console.log("ğŸ“ [ChatService] User message:", userMessage);
      console.log("ğŸ“ [ChatService] History length:", history.length);
      
      const messages = PromptService.buildMessages(userMessage, history);
      
      console.log("ğŸ“ [ChatService] Built messages:", messages.length);
      console.log("ğŸ“ [ChatService] Messages:", JSON.stringify(messages, null, 2));
      
      Logger.debug("Building messages for LLM", {
        totalMessages: messages.length,
        systemMessage: messages[0]?.role === "system",
        historyMessages: messages.length - 2, // å‡å» system å’Œå½“å‰ user message
        currentUserMessage: userMessage.substring(0, 50),
      });
      
      console.log("ğŸ“ [ChatService] æº–å‚™èª¿ç”¨ LLM client");
      
      // æ·»åŠ è¶…æ™‚è™•ç†
      const timeoutPromise = new Promise<string>((_, reject) => {
        setTimeout(() => reject(new Error("Request timeout")), 30000); // 30 ç§’è¶…æ™‚
      });

      console.log("ğŸ“ [ChatService] èª¿ç”¨ this.llmClient.chat()");
      const responsePromise = this.llmClient.chat(messages);
      console.log("â³ [ChatService] ç­‰å¾… LLM å›æ‡‰...");
      
      const response = await Promise.race([responsePromise, timeoutPromise]);
      
      console.log("âœ… [ChatService] LLM å›æ‡‰æ”¶åˆ°:", response);
      
      Logger.debug("LLM response generated", {
        responseLength: response.length,
        responsePreview: response.substring(0, 100),
      });
      
      return response;
    } catch (error) {
      console.error("âŒ [ChatService] ç”Ÿæˆå›æ‡‰å¤±æ•—");
      console.error("âŒ [ChatService] Error:", error);
      console.error("âŒ [ChatService] Error message:", error instanceof Error ? error.message : String(error));
      console.error("âŒ [ChatService] Error stack:", error instanceof Error ? error.stack : undefined);
      
      Logger.error("Failed to generate response", { 
        error,
        errorMessage: error instanceof Error ? error.message : String(error),
        userMessage: userMessage.substring(0, 50),
      });
      const errorMessage = handleLLMError(error);
      console.log("âš ï¸ [ChatService] è¿”å› fallback å›æ‡‰:", errorMessage);
      return errorMessage;
    }
  }

  getWelcomeMessage(): string {
    return PromptService.getWelcomeMessage();
  }

  getHelpMessage(): string {
    return PromptService.getHelpMessage();
  }
}

